{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "DATASET = \"data.xlsx\"\n",
    "DATASET_SHEET = \"data\"\n",
    "QUESTION_DEFINITIONS = \"question_definitions.xlsx\"\n",
    "QUESTION_DEFINITIONS_SHEET = \"Sheet1\"\n",
    "ADDITIONAL_STOP_WORDS = [\"nil\",\"na\"]\n",
    "WORD_FREQUENCIES_TOP = 10\n",
    "RESPONDENT = \"Respondent\"\n",
    "EPV_CONTRACT_DURATION_IN_YEARS = 7\n",
    "EPV_HOSTING_COST = {\n",
    "    \"small\": 1,\n",
    "    \"medium\": 2,\n",
    "    \"large\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# import nltk\n",
    "# nltk.download(['stopwords','vader_lexicon'])\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# matplotlib Configurations\n",
    "params = {\n",
    "    'figure.figsize': (10, 5),\n",
    "    'font.size': 12\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "def create_graph(x, y, title=\"\", xlabel=\"\", ylabel= \"\", type=\"\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    x = truncate_text(x)\n",
    "    \n",
    "    if type == \"bar\":\n",
    "        ax.barh(x, y)\n",
    "    else:\n",
    "        ax.plot(x, y, \"x-\")\n",
    "    \n",
    "    ax.set_title(title, pad=50)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    # Hide the top and right borders\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    # Show the values on top of the point / bar\n",
    "    for i, v in enumerate(y):\n",
    "        ax.text(v + .05, i, str(v), ha='left', va='center')\n",
    "    plt.show()\n",
    "    \n",
    "def html_print(text):\n",
    "    display(HTML(text))\n",
    "    \n",
    "def generate_word_list(s):\n",
    "    processed_wordlist = []\n",
    "    # Convert string to lowercase\n",
    "    s = s.lower()\n",
    "    wordlist = s.split()\n",
    "    for word in wordlist:\n",
    "        # Avoid processing stop words\n",
    "        if word not in stopWords and word not in ADDITIONAL_STOP_WORDS:\n",
    "            # Remove symbols\n",
    "            word = re.sub(r'[^\\w]', \" \", word)\n",
    "            processed_words = word.split()\n",
    "            for w in processed_words:\n",
    "                # Not adding words with only one alphabet\n",
    "                if len(w) > 1:\n",
    "                    processed_wordlist.append(w)\n",
    "    return processed_wordlist\n",
    "\n",
    "def word_frequencies(wordlist):\n",
    "    ps = PorterStemmer()\n",
    "    wordfreq = []\n",
    "    wordstems = []\n",
    "    for w in wordlist:\n",
    "        wordfreq.append(wordlist.count(w))\n",
    "        wordstems.append(ps.stem(w))\n",
    "\n",
    "    result = list(zip(wordlist, wordfreq, wordstems))\n",
    "    # Remove duplicates\n",
    "    result = list(dict.fromkeys(result))\n",
    "    \n",
    "    # Sort the list based on stems\n",
    "    result = sorted(result, key=lambda x:x[2])\n",
    "    # Group similar words based on stems\n",
    "    previousStem = \"\"\n",
    "    tpl = (\"\", 0)\n",
    "    newResult = [(\", \".join(m[0] for m in result if r[2] == m[2]), sum(k[1] for k in result if r[2] == k[2])) for r in result]\n",
    "    newResult = list(set(newResult)) # remove duplicates\n",
    "            \n",
    "    # Sort the list\n",
    "    newResult = sorted(newResult, key=lambda x:x[1], reverse=True)\n",
    "    if len(newResult) > WORD_FREQUENCIES_TOP:\n",
    "        return newResult[:WORD_FREQUENCIES_TOP]\n",
    "    else:\n",
    "        return newResult\n",
    "\n",
    "def sort_tuples(t, reverse=False):\n",
    "    return sorted(t, key=lambda k: k[1], reverse=reverse)\n",
    "\n",
    "def word_frequency_graph(wf, graph_title):\n",
    "    if len(wf) > 0:\n",
    "        x, y = zip(*sort_tuples(wf))\n",
    "        create_graph(x, y, graph_title, \"Word\", \"Count\", \"bar\")\n",
    "        \n",
    "def k_means_clustering(titles, documents):\n",
    "    # Represent each document as a vector\n",
    "    vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Using elbow method to determine the number of clusters\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(2,10)\n",
    "    best_cluster_no = 0 # Best cluster number which you will get\n",
    "    previous_silh_avg = 0.0\n",
    "\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
    "        km = km.fit(X)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "        # To determine the most optimal number of clusters automatically (without looking at the graph)\n",
    "        cluster_labels = km.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        if silhouette_avg > previous_silh_avg:\n",
    "            previous_silh_avg = silhouette_avg\n",
    "            best_cluster_no = k\n",
    "\n",
    "    # create_graph(K, Sum_of_squared_distances,\"Elbow Method For Optimal k\", \"k\", \"Sum of Squared Distances\")\n",
    "    print(\"Total Clusters: \" + str(best_cluster_no))\n",
    "    \n",
    "    # Perform K-Means clustering\n",
    "    model = KMeans(n_clusters=best_cluster_no, init='k-means++', max_iter=200, n_init=10)\n",
    "    model.fit(X)\n",
    "    labels=model.labels_\n",
    "    # cls=pd.DataFrame(list(zip(titles,labels)),columns=['title','cluster'])\n",
    "    # print(cls.sort_values(by=['cluster']))\n",
    "    return list(zip(titles,labels))\n",
    "\n",
    "now = str(datetime.datetime.now()).split(\" \")\n",
    "folder_name = now[0] + \"_\" + now[1].split(\".\")[0].replace(\":\", \"-\")\n",
    "if Path('results\\\\' + folder_name).is_dir() == False:\n",
    "    Path('results\\\\' + folder_name).mkdir(parents=True, exist_ok=True)\n",
    "def save_to_spreadsheet(data, question_no):\n",
    "    s_df = pd.DataFrame(data=data)\n",
    "    s_df.to_excel(\"results\\\\\" + folder_name + \"\\Q\" + str(question_no) + \".xlsx\", sheet_name=\"Q\" + str(question_no), index=False)\n",
    "    print(\"\\nRefer to 'results\\\\\" + folder_name + \"\\Q\" + str(question_no) + \".xlsx' for the processed data.\")\n",
    "    \n",
    "def truncate_text(text_tpl):\n",
    "    processed_text_list = []\n",
    "    for t in text_tpl:\n",
    "        s = t\n",
    "        words = s.split(\" \")\n",
    "        if len(words) > 5:\n",
    "            s = \" \".join(words[:5])\n",
    "            s += \" ...\"\n",
    "        processed_text_list.append(s)\n",
    "    return tuple(processed_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(open(DATASET,'rb'), sheet_name=DATASET_SHEET, keep_default_na=False)\n",
    "qd = pd.read_excel(open(QUESTION_DEFINITIONS,'rb'), sheet_name=QUESTION_DEFINITIONS_SHEET, keep_default_na=False)\n",
    "\n",
    "print(\"Total Responses: \" + str(len(df.index)))\n",
    "    \n",
    "question_type_full_name = {\n",
    "    \"sc\": \"Single Choice\",\n",
    "    \"mc\": \"Multiple Choice\",\n",
    "    \"txt\": \"Free Text\",\n",
    "    \"int\": \"Number\",\n",
    "    \"epv\": \"EPV\",\n",
    "    \"wf\": \"Website Features\"\n",
    "}\n",
    "question_no = 0\n",
    "for index, row in qd.iterrows():\n",
    "    question = row[\"Question\"]\n",
    "    question_type = row[\"Type\"]\n",
    "    question_no += 1\n",
    "    html_print(\"<h3>[Q\" + str(question_no) + \"] \" + question + \"</h3>\")\n",
    "    html_print(\"<i>\" + question_type_full_name[question_type] + \"</i>\")\n",
    "    html_print(\"<hr/>\")\n",
    "    if question_type == \"sc\":\n",
    "        df[question] = df[question].replace(r'^\\s*$', \"(Empty)\", regex=True)\n",
    "        grpby = df.groupby(question)[question].count()\n",
    "        x,y = zip(*sort_tuples(grpby.items()))\n",
    "        create_graph(x, y, \"\", \"Values\", \"Count\", \"bar\")\n",
    "        \n",
    "        save_to_spreadsheet({ \"Respondent\": df[RESPONDENT], \"Values\": df[question] }, question_no)\n",
    "    elif question_type == \"mc\":\n",
    "        df[question] = df[question].replace(r'^\\s*$', \"(Empty)\", regex=True)\n",
    "        obj = {}\n",
    "        respondents = []\n",
    "        values = []\n",
    "        index = 0\n",
    "        for row in df[question]:\n",
    "            selected_options = str(row).split(\";\")\n",
    "            for option in selected_options:\n",
    "                if option not in obj.keys():\n",
    "                    obj[option] = 1\n",
    "                else:\n",
    "                    obj[option] += 1\n",
    "                    \n",
    "                respondents.append(df[RESPONDENT][index])\n",
    "                values.append(option)\n",
    "            \n",
    "            index += 1\n",
    "                \n",
    "        x, y = zip(*sort_tuples(obj.items()))\n",
    "        create_graph(x, y, \"\", \"Values\", \"Count\", \"bar\")\n",
    "        \n",
    "        save_to_spreadsheet({ \"Respondent\": respondents, \"Values\": values }, question_no)\n",
    "    elif question_type == \"int\":\n",
    "        df[question] = df[question].replace(r'^\\s*$', 0, regex=True)\n",
    "        grpby = df.groupby(question)[question].count()\n",
    "        print(\"Min: \" + str(df[question].min()))\n",
    "        print(\"Max: \" + str(df[question].max()))\n",
    "        print(\"Total: \" + str(df[question].sum()))\n",
    "        x,y = zip(*sort_tuples(grpby.items()))\n",
    "        x = list(map(str, x)) # Need to convert all integers to string for the sorting in the graph to work properly\n",
    "        create_graph(x, y, \"\", \"Values\", \"Count\", \"bar\")\n",
    "        \n",
    "        save_to_spreadsheet({ \"Respondent\": df[RESPONDENT], \"Values\": df[question] }, question_no)\n",
    "    elif question_type == \"txt\":\n",
    "        sa = SentimentIntensityAnalyzer()\n",
    "        pos_list = []\n",
    "        neg_list = []\n",
    "        neu_list = []\n",
    "        consolidated_data = {\n",
    "            \"Respondent\": [],\n",
    "            \"Text\": [],\n",
    "            \"Sentiment\": [],\n",
    "            \"Score\": [],\n",
    "        }\n",
    "        \n",
    "        # Sentiment Analysis and Sorting of Text\n",
    "        index = 0\n",
    "        for row in df[question]:\n",
    "            consolidated_data[\"Respondent\"].append(df[RESPONDENT][index])\n",
    "            \n",
    "            if not row:\n",
    "                row = \"(Empty)\"\n",
    "            \n",
    "            sa_result = sa.polarity_scores(row)\n",
    "            pos, neg, neu = itemgetter('pos', 'neg', 'neu')(sa_result)\n",
    "            consolidated_data[\"Text\"].append(row)\n",
    "            if pos >= neg + neu:\n",
    "                pos_list.append(row)\n",
    "                consolidated_data[\"Sentiment\"].append(\"Positive\")\n",
    "                consolidated_data[\"Score\"].append(pos)\n",
    "            elif neg >= pos + neu:\n",
    "                neg_list.append(row)\n",
    "                consolidated_data[\"Sentiment\"].append(\"Negative\")\n",
    "                consolidated_data[\"Score\"].append(neg)\n",
    "            else:\n",
    "                neu_list.append(row)\n",
    "                consolidated_data[\"Sentiment\"].append(\"Neutral\")\n",
    "                consolidated_data[\"Score\"].append(neu)\n",
    "\n",
    "            index += 1\n",
    "        \n",
    "        x = [\"Postive\", \"Neutral\", \"Negative\"]\n",
    "        y = [len(pos_list), len(neu_list), len(neg_list)]\n",
    "        create_graph(x, y, \"Number of Responses by Sentiments\", \"Sentiments\", \"Count\", \"bar\")\n",
    "        \n",
    "        # Highlighting the words with high frequencies\n",
    "#         pos_wf = word_frequencies(generate_word_list(\" \".join(pos_list)))\n",
    "#         word_frequency_graph(pos_wf, \"Word Frequencies - Positive\")\n",
    "#         print(\"\\n\")\n",
    "#         neu_wf = word_frequencies(generate_word_list(\" \".join(neu_list)))\n",
    "#         word_frequency_graph(neu_wf, \"Word Frequencies - Neutral\")\n",
    "#         print(\"\\n\")\n",
    "#         neg_wf = word_frequencies(generate_word_list(\" \".join(neg_list)))\n",
    "#         word_frequency_graph(neg_wf, \"Word Frequencies - Negative\")\n",
    "        \n",
    "        # Tag text if they contain words with high frequencies\n",
    "#         index = -1\n",
    "#         for row in consolidated_data[\"Text\"]:\n",
    "#             index += 1\n",
    "#             if consolidated_data[\"Sentiment\"][index] == \"Positive\":\n",
    "#                 wf = pos_wf\n",
    "#             elif consolidated_data[\"Sentiment\"][index] == \"Negative\":\n",
    "#                 wf = neg_wf\n",
    "#             elif consolidated_data[\"Sentiment\"][index] == \"Neutral\":\n",
    "#                 wf = neu_wf\n",
    "            \n",
    "#             containWordsWithHighFrequencies = \"\"\n",
    "#             text_lowercase = row.lower()\n",
    "#             for w in wf:\n",
    "#                 if any(ww in text_lowercase for ww in w[0].split(\", \")):\n",
    "#                     if containWordsWithHighFrequencies != \"\":\n",
    "#                         containWordsWithHighFrequencies += \", \" + w[0]\n",
    "#                     else:\n",
    "#                         containWordsWithHighFrequencies = w[0]\n",
    "#             if containWordsWithHighFrequencies != \"\":\n",
    "#                 consolidated_data[\"ContainWordsWithHighFrequencies\"].append(containWordsWithHighFrequencies)\n",
    "#             else:\n",
    "#                 consolidated_data[\"ContainWordsWithHighFrequencies\"].append(\"-\")\n",
    "        \n",
    "        # K-Means Clustering\n",
    "        unique_records = len(list(set([(t, sum(1 for t2 in consolidated_data[\"Text\"] if t2 == t)) for t in consolidated_data[\"Text\"]])))\n",
    "        if unique_records < 10:\n",
    "            print(\"There are less than 10 unique records for text clustering.\")\n",
    "        else:\n",
    "            cluster_list = k_means_clustering(consolidated_data[\"Respondent\"], consolidated_data[\"Text\"])\n",
    "            respondents, clusters = zip(*cluster_list)\n",
    "            clusters = list(clusters)\n",
    "            num_of_clusters = max(clusters) + 1\n",
    "            text_clusters = list(zip(consolidated_data[\"Text\"], clusters))\n",
    "            text_clusters = sort_tuples(list(set([(\" \".join([t2[0] for t2 in text_clusters if t2[1] == t[1]]), t[1]) for t in text_clusters])), False)\n",
    "            for t in text_clusters:\n",
    "                wf = word_frequencies(generate_word_list(t[0]))\n",
    "                html_print(\"<b>Word Frequencies for Cluster \" + str(t[1]) + \"</b>\")\n",
    "                for w in wf:\n",
    "                    print(w[0] + \"[\" + str(w[1]) + \"]\")\n",
    "            consolidated_data[\"Clusters\"] = clusters\n",
    "        \n",
    "        save_to_spreadsheet(consolidated_data, question_no)\n",
    "        \n",
    "    elif question_type == \"epv\":\n",
    "        fdf = df.filter(like=question)\n",
    "        d = {\"Respondent\": [], \"Small\": [], \"Medium\": [], \"Large\": [], \"EpvDevelopment\": [], \"EpvMaintenance\": [], \"EpvHosting\": [], \"TotalEpv\": []}\n",
    "        \n",
    "        for index, row in fdf.iterrows():\n",
    "            tier_count = {\n",
    "                \"small\": 0,\n",
    "                \"medium\": 0,\n",
    "                \"large\": 0,\n",
    "                \"(empty)\": 0\n",
    "            }\n",
    "            epvDevelopment = 0\n",
    "            epvMaintenance = 0\n",
    "            epvHosting = 0\n",
    "            for i in range(0,len(fdf.columns)):\n",
    "                if row[i]:\n",
    "                    values = row[i].split(\";\")\n",
    "                    if not values[0]:\n",
    "                        values[0] = \"(Empty)\"\n",
    "                    for i in range(1,4):\n",
    "                        try:\n",
    "                            if not values[i]:\n",
    "                                values[i] = 0\n",
    "                            elif not isinstance(int(values[i].replace(\"$\",\"\").replace(\",\",\"\")), int):\n",
    "                                values[i] = 0\n",
    "                        except ValueError:\n",
    "                            values[i] = 0\n",
    "                else:\n",
    "                    values = [\"(Empty)\",\"0\",\"0\",\"0\"]\n",
    "                \n",
    "                tier = values[0].lower()\n",
    "                if tier in EPV_HOSTING_COST.keys():\n",
    "                    tier_count[tier] += int(str(values[1]).replace(\"$\",\"\").replace(\",\",\"\"))\n",
    "                    epvDevelopment += int(str(values[2]).replace(\"$\",\"\").replace(\",\",\"\"))\n",
    "                    epvMaintenance += int(str(values[3]).replace(\"$\",\"\").replace(\",\",\"\")) * EPV_CONTRACT_DURATION_IN_YEARS\n",
    "                    # '12' refers to 12 months\n",
    "                    epvHosting += int(str(values[1]).replace(\"$\",\"\").replace(\",\",\"\")) * EPV_HOSTING_COST[tier] * 12 * EPV_CONTRACT_DURATION_IN_YEARS\n",
    "            \n",
    "            d[\"Respondent\"].append(df[RESPONDENT][index])\n",
    "            d[\"Small\"].append(tier_count[\"small\"])\n",
    "            d[\"Medium\"].append(tier_count[\"medium\"])\n",
    "            d[\"Large\"].append(tier_count[\"large\"])\n",
    "            d[\"EpvDevelopment\"].append(epvDevelopment)\n",
    "            d[\"EpvMaintenance\"].append(epvMaintenance)\n",
    "            d[\"EpvHosting\"].append(epvHosting)\n",
    "            d[\"TotalEpv\"].append(epvDevelopment + epvMaintenance + epvHosting)\n",
    "        epv_df = pd.DataFrame(data=d)\n",
    "        print(\"Grand Total of EPV: \" + str(epv_df[\"TotalEpv\"].sum()))\n",
    "        x, y = zip(*sort_tuples(zip(d[\"Respondent\"], d[\"TotalEpv\"])))\n",
    "        create_graph(x, y,\"Total EPV by Respondent\", \"Respondent\", \"Total EPV\", \"bar\")\n",
    "        x, y = zip(*sort_tuples(zip(d[\"Respondent\"], d[\"Small\"])))\n",
    "        create_graph(x, y,\"Small Websites by Respondent\", \"Respondent\", \"Number of Websites\", \"bar\")\n",
    "        x, y = zip(*sort_tuples(zip(d[\"Respondent\"], d[\"Medium\"])))\n",
    "        create_graph(x, y,\"Medium Websites by Respondent\", \"Respondent\", \"Number of Websites\", \"bar\")\n",
    "        x, y = zip(*sort_tuples(zip(d[\"Respondent\"], d[\"Large\"])))\n",
    "        create_graph(x, y,\"Large Websites by Respondent\", \"Respondent\", \"Number of Websites\", \"bar\")\n",
    "        \n",
    "        save_to_spreadsheet(d, question_no)\n",
    "        \n",
    "    elif question_type == \"wf\":\n",
    "        fdf = df.filter(like=question)\n",
    "        d = {\"Respondent\": [], \"Features\": [], \"Classification\": [], \"NumberOfWebsites\": []}\n",
    "        \n",
    "        for index, row in fdf.iterrows():\n",
    "            for i in range(0,len(fdf.columns)):\n",
    "                d[\"Respondent\"].append(df[RESPONDENT][index])\n",
    "                if row[i]:\n",
    "                    values = row[i].split(\";\")\n",
    "                    if not values[0]:\n",
    "                        values[0] = \"(Empty)\"\n",
    "                    if not values[1]:\n",
    "                        values[1] = \"(Empty)\"\n",
    "                    if not values[2]:\n",
    "                        values[2] = \"0\"\n",
    "                else:\n",
    "                    values = [\"(Empty)\",\"(Empty)\",\"0\"]\n",
    "                d[\"Features\"].append(values[0].replace(\" \", \"\"))\n",
    "                d[\"Classification\"].append(values[1].replace(\" \", \"\"))\n",
    "                d[\"NumberOfWebsites\"].append(int(values[2]))\n",
    "                \n",
    "        feature_and_noOfWebsites = list(zip(d[\"Features\"], d[\"NumberOfWebsites\"]))\n",
    "        feature_and_noOfWebsites = list(set([(f[0], sum(int(g[1]) for g in feature_and_noOfWebsites if g[0] == f[0])) for f in feature_and_noOfWebsites]))\n",
    "        x, y = zip(*sort_tuples(feature_and_noOfWebsites))\n",
    "        create_graph(x, y, \"Number of Websites by Features\", \"Features\", \"Number of Websites\", \"bar\")\n",
    "        \n",
    "        classification_and_noOfWebsites = list(zip(d[\"Classification\"], d[\"NumberOfWebsites\"]))\n",
    "        classification_and_noOfWebsites = list(set([(c[0], sum(int(g[1]) for g in classification_and_noOfWebsites if g[0] == c[0])) for c in classification_and_noOfWebsites]))\n",
    "        x, y = zip(*sort_tuples(classification_and_noOfWebsites))\n",
    "        create_graph(x, y, \"Number of Websites by Classification\", \"Classification\", \"Number of Websites\", \"bar\")\n",
    "                \n",
    "        save_to_spreadsheet(d, question_no)\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
