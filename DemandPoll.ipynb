{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "DATASET = \"sample_data.xlsx\"\n",
    "DATASET_SHEET = \"Sheet1\"\n",
    "QUESTION_DEFINITIONS = \"question_definitions.xlsx\"\n",
    "QUESTION_DEFINITIONS_SHEET = \"Sheet1\"\n",
    "ADDITIONAL_STOP_WORDS = [\"nil\"]\n",
    "WORD_FREQUENCIES_TOP = 10\n",
    "RESPONDER = \"Responder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# import nltk\n",
    "# nltk.download(['stopwords','vader_lexicon'])\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# matplotlib Configurations\n",
    "params = {\n",
    "    'figure.figsize': (15, 5),\n",
    "    'font.size': 16\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "def create_graph(x, y, title=\"\", xlabel=\"\", ylabel= \"\", type=\"\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    if type == \"bar\":\n",
    "        ax.bar(x, y)\n",
    "    else:\n",
    "        ax.plot(x, y)\n",
    "    \n",
    "    ax.set_title(title, pad=50)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True)) # Ensure that x-axis is an integer where applicable\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True)) # Ensure that y-axis is an integer where applicable\n",
    "    # Hide the top and right borders\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    # Show the values on top of the point / bar\n",
    "    for i, v in enumerate(y):\n",
    "        ax.text(i, v + .05, str(v), ha='center', va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "def html_print(text):\n",
    "    display(HTML(text))\n",
    "    \n",
    "def generate_word_list(s):\n",
    "    processed_wordlist = []\n",
    "    # Convert string to lowercase\n",
    "    s = s.lower()\n",
    "    wordlist = s.split()\n",
    "    for word in wordlist:\n",
    "        # Avoid processing stop words\n",
    "        if word not in stopWords and word not in ADDITIONAL_STOP_WORDS:\n",
    "            # Remove symbols\n",
    "            word = re.sub(r'[^\\w]', \" \", word)\n",
    "            processed_words = word.split()\n",
    "            for w in processed_words:\n",
    "                # Not adding words with only one alphabet\n",
    "                if len(w) > 1:\n",
    "                    processed_wordlist.append(w)\n",
    "    return processed_wordlist\n",
    "\n",
    "def word_frequencies(wordlist):\n",
    "    ps = PorterStemmer()\n",
    "    wordfreq = []\n",
    "    wordstems = []\n",
    "    for w in wordlist:\n",
    "        wordfreq.append(wordlist.count(w))\n",
    "        wordstems.append(ps.stem(w))\n",
    "\n",
    "    result = list(zip(wordlist, wordfreq, wordstems))\n",
    "    # Remove duplicates\n",
    "    result = list(dict.fromkeys(result))\n",
    "    \n",
    "    # Sort the list based on stems\n",
    "    result = sorted(result, key=lambda x:x[2])\n",
    "    # Group similar words based on stems\n",
    "    previousStem = \"\"\n",
    "    tpl = (\"\", 0)\n",
    "    newResult = [(\", \".join(m[0] for m in result if r[2] == m[2]), sum(k[1] for k in result if r[2] == k[2])) for r in result]\n",
    "    newResult = list(set(newResult)) # remove duplicates\n",
    "            \n",
    "    # Sort the list\n",
    "    newResult = sorted(newResult, key=lambda x:x[1], reverse=True)\n",
    "    if len(newResult) > WORD_FREQUENCIES_TOP:\n",
    "        return newResult[:WORD_FREQUENCIES_TOP]\n",
    "    else:\n",
    "        return newResult\n",
    "\n",
    "def process_tuples(t):\n",
    "    return sorted(t, key=lambda k: k[1], reverse=True)\n",
    "\n",
    "def word_frequency_graph(wf, graph_title):\n",
    "    if len(wf) > 0:\n",
    "        x, y = zip(*process_tuples(wf))\n",
    "        create_graph(x, y, graph_title, \"Word\", \"Count\", \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xl = pd.ExcelFile(DATASET)\n",
    "df = xl.parse(DATASET_SHEET)\n",
    "xl = pd.ExcelFile(QUESTION_DEFINITIONS)\n",
    "qd = xl.parse(QUESTION_DEFINITIONS_SHEET)\n",
    "\n",
    "if Path('results').is_dir() == False:\n",
    "    Path(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "question_count = 0\n",
    "for index, row in qd.iterrows():\n",
    "    question = row[\"Question\"]\n",
    "    question_type = row[\"Type\"]\n",
    "    question_count += 1\n",
    "    html_print(\"<h3>Q\" + str(question_count) + \": \" + question + \"</h3>\")\n",
    "    html_print(\"<hr/>\")\n",
    "    if question_type == \"sc\":\n",
    "        grpby = df.groupby(question)[question].count()\n",
    "        x,y = zip(*process_tuples(grpby.items()))\n",
    "        create_graph(x, y, \"\", \"Values\", \"Count\", \"bar\")\n",
    "    elif question_type == \"mc\":\n",
    "        obj = {}\n",
    "        for row in df[question]:\n",
    "            selected_options = str(row).split(\";\")\n",
    "            for option in selected_options:\n",
    "                if option not in obj.keys():\n",
    "                    obj[option] = 1\n",
    "                else:\n",
    "                    obj[option] += 1\n",
    "        x, y = zip(*process_tuples(obj.items()))\n",
    "        create_graph(x, y, \"\", \"Values\", \"Count\", \"bar\")\n",
    "    elif question_type == \"int\":\n",
    "        grpby = df.groupby(question)[question].count()\n",
    "        print(\"Min: \" + str(df[question].min()))\n",
    "        print(\"Max: \" + str(df[question].max()))\n",
    "        print(\"Total: \" + str(df[question].sum()))\n",
    "        x,y = zip(*process_tuples(grpby.items()))\n",
    "        x = list(map(str, x)) # Need to convert all integers to string for the sorting in the graph to work properly\n",
    "        create_graph(x, y, \"\", \"Values\", \"Count\", \"bar\")\n",
    "    elif question_type == \"txt\":\n",
    "        sa = SentimentIntensityAnalyzer()\n",
    "        pos_list = []\n",
    "        neg_list = []\n",
    "        neu_list = []\n",
    "        consolidated_data = {\"Responder\": [], \"Text\": [], \"Sentiment\": [], \"Score\": [], \"ContainWordsWithHighFrequencies\": []}\n",
    "        \n",
    "        # Sentiment Analysis and Sorting of Text\n",
    "        index = 0\n",
    "        for row in df[question]:\n",
    "            consolidated_data[\"Responder\"].append(df[RESPONDER][index])\n",
    "            if row != \"\":\n",
    "                sa_result = sa.polarity_scores(row)\n",
    "                pos, neg, neu = itemgetter('pos', 'neg', 'neu')(sa_result)\n",
    "                consolidated_data[\"Text\"].append(row)\n",
    "                if pos >= neg + neu:\n",
    "                    pos_list.append(row)\n",
    "                    consolidated_data[\"Sentiment\"].append(\"Positive\")\n",
    "                    consolidated_data[\"Score\"].append(pos)\n",
    "                elif neg >= pos + neu:\n",
    "                    neg_list.append(row)\n",
    "                    consolidated_data[\"Sentiment\"].append(\"Negative\")\n",
    "                    consolidated_data[\"Score\"].append(neg)\n",
    "                else:\n",
    "                    neu_list.append(row)\n",
    "                    consolidated_data[\"Sentiment\"].append(\"Neutral\")\n",
    "                    consolidated_data[\"Score\"].append(neu)\n",
    "            else:\n",
    "                neu_list.append(\"nil\")\n",
    "                consolidated_data[\"Text\"].append(\"nil\")\n",
    "                consolidated_data[\"Sentiment\"].append(\"Neutral\")\n",
    "                consolidated_data[\"Score\"].append(1.0)\n",
    "            index += 1\n",
    "        \n",
    "        # Highlighting the words with high frequencies\n",
    "        x = [\"Postive\", \"Neutral\", \"Negative\"]\n",
    "        y = [len(pos_list), len(neu_list), len(neg_list)]\n",
    "        create_graph(x, y, \"Number of Responses by Sentiments\", \"Sentiments\", \"Count\", \"bar\")\n",
    "        pos_wf = word_frequencies(generate_word_list(\" \".join(pos_list)))\n",
    "        word_frequency_graph(pos_wf, \"Word Frequencies - Positive\")\n",
    "        print(\"\\n\")\n",
    "        neu_wf = word_frequencies(generate_word_list(\" \".join(neu_list)))\n",
    "        word_frequency_graph(neu_wf, \"Word Frequencies - Neutral\")\n",
    "        print(\"\\n\")\n",
    "        neg_wf = word_frequencies(generate_word_list(\" \".join(neg_list)))\n",
    "        word_frequency_graph(neg_wf, \"Word Frequencies - Negative\")\n",
    "        \n",
    "        # Tag text if they contain words with high frequencies\n",
    "        index = -1\n",
    "        for row in consolidated_data[\"Text\"]:\n",
    "            index += 1\n",
    "            if consolidated_data[\"Sentiment\"][index] == \"Positive\":\n",
    "                wf = pos_wf\n",
    "            elif consolidated_data[\"Sentiment\"][index] == \"Negative\":\n",
    "                wf = neg_wf\n",
    "            elif consolidated_data[\"Sentiment\"][index] == \"Neutral\":\n",
    "                wf = neu_wf\n",
    "            \n",
    "            containWordsWithHighFrequencies = \"\"\n",
    "            text_lowercase = row.lower()\n",
    "            for w in wf:\n",
    "                if any(ww in text_lowercase for ww in w[0].split(\", \")):\n",
    "                    if containWordsWithHighFrequencies != \"\":\n",
    "                        containWordsWithHighFrequencies += \", \" + w[0]\n",
    "                    else:\n",
    "                        containWordsWithHighFrequencies = w[0]\n",
    "            if containWordsWithHighFrequencies != \"\":\n",
    "                consolidated_data[\"ContainWordsWithHighFrequencies\"].append(containWordsWithHighFrequencies)\n",
    "            else:\n",
    "                consolidated_data[\"ContainWordsWithHighFrequencies\"].append(\"-\")\n",
    "        \n",
    "        # Store results in a Excel spreadsheet\n",
    "        sentiment_df = pd.DataFrame(data=consolidated_data)\n",
    "        sentiment_df.to_excel(\"results\\Q\" + str(question_count) + \".xlsx\", index=False)\n",
    "    elif question_type == \"epv\":\n",
    "        fdf = df.filter(like=question)\n",
    "        d = {\"Responder\": [], \"Small\": [], \"Medium\": [], \"Large\": [], \"EpvDevelopment\": [], \"EpvMaintenance\": [], \"EpvHosting\": [], \"TotalEpv\": []}\n",
    "        hosting_cost = {\n",
    "            \"Small\": 1,\n",
    "            \"Medium\": 2,\n",
    "            \"Large\": 3\n",
    "        }\n",
    "        for index, row in fdf.iterrows():\n",
    "            tier_count = {\n",
    "                \"Small\": 0,\n",
    "                \"Medium\": 0,\n",
    "                \"Large\": 0\n",
    "            }\n",
    "            epvDevelopment = 0\n",
    "            epvMaintenance = 0\n",
    "            epvHosting = 0\n",
    "            for i in range(0,len(fdf.columns)):\n",
    "                values = row[i].split(\";\")\n",
    "                tier_count[values[0]] += int(values[1])\n",
    "                epvDevelopment += int(values[2])\n",
    "                epvMaintenance += int(values[3])\n",
    "                epvHosting += int(values[1]) * hosting_cost[values[0]]\n",
    "            \n",
    "            d[\"Responder\"].append(df[RESPONDER][index])\n",
    "            d[\"Small\"].append(tier_count[\"Small\"])\n",
    "            d[\"Medium\"].append(tier_count[\"Medium\"])\n",
    "            d[\"Large\"].append(tier_count[\"Large\"])\n",
    "            d[\"EpvDevelopment\"].append(epvDevelopment)\n",
    "            d[\"EpvMaintenance\"].append(epvMaintenance)\n",
    "            d[\"EpvHosting\"].append(epvHosting)\n",
    "            d[\"TotalEpv\"].append(epvDevelopment + epvMaintenance + epvHosting)\n",
    "        epv_df = pd.DataFrame(data=d)\n",
    "        epv_df.to_excel(\"results\\Q\" + str(question_count) + \".xlsx\", index=False)\n",
    "        print(\"Grand Total of EPV: \" + str(epv_df[\"TotalEpv\"].sum()))\n",
    "        x, y = zip(*process_tuples(zip(d[\"Responder\"], d[\"TotalEpv\"])))\n",
    "        create_graph(x, y,\"Total EPV by Responder\", \"Responder\", \"Total EPV\", \"bar\")\n",
    "        x, y = zip(*process_tuples(zip(d[\"Responder\"], d[\"Small\"])))\n",
    "        create_graph(x, y,\"Small Websites by Responder\", \"Responder\", \"Number of Websites\", \"bar\")\n",
    "        x, y = zip(*process_tuples(zip(d[\"Responder\"], d[\"Medium\"])))\n",
    "        create_graph(x, y,\"Medium Websites by Responder\", \"Responder\", \"Number of Websites\", \"bar\")\n",
    "        x, y = zip(*process_tuples(zip(d[\"Responder\"], d[\"Large\"])))\n",
    "        create_graph(x, y,\"Large Websites by Responder\", \"Responder\", \"Number of Websites\", \"bar\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
